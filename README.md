# Arabic_CUB-200-2011-Dataset
Translated Caltech-UCSD Birds-200-2011 (CUB-200-2011) dataset from English to Arabic

## Dataset
While the Arabic language has recently become the fourth most used language on the web, few studies deal with texts in Arabic. This is because of the Arabic language's morphological complexity, the lack of publicly available language, and the use of Modern Standard Arabic (MSA) and different dialects. Due to the lack of Arabic datasets available to the public, we collect and translate text of the CUB-200-2011 dataset.
After collecting [Caltech UCSD Birds dataset (CUB-200-2011)](http://www.vision.caltech.edu/visipedia/CUB-200.html) 
we translated CUB dataset captions from English to Arabic language as illustrated
![fig 10 crop](https://user-images.githubusercontent.com/43410503/171548574-d05efdf0-ceab-4a39-b8f4-86e341047fdf.png)
Caltech-UCSD Birds-200-2011 (CUB-200-2011) is an extended version of the CUB-200 dataset, with roughly double the number of images per class and new part location annotations.

- Number of categories: 200

- Number of images: 11,788

- Annotations per image: 15 Part Locations, 312 Binary Attributes, 1 Bounding Box

## Data files

To download the data click on the following links:

   1. Images and annotations: Download the [birds](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) image data. Extract them to `data/birds/`
               
    
   2. Download our Arabic texts and preprocessed metadata for [birds](https://drive.google.com/drive/folders/12dMXzrtVON2qBLjyD38xmvipwuUnEvSu?usp=sharing)
## Examples generated by our Arabic_CUB-200-2011-Dataset
We use our dataset to generate Photo-realistic from Arabic-text description in the Accepted [RESEARCH-ARTICLE](https://dl.acm.org/doi/10.1145/3490504).

Our proposed model provides a new model for converting Arabic text into realistic images. A mutation occurs in the use of Arabic as the first use to convert Arabic texts into real images. The proposed model boosts a good reported inception score by 3.42 ± .05 on the CUB dataset. The suggested approach sets a new standard in which the Arabic text is converted into realistic images.
![fig 12](https://user-images.githubusercontent.com/43410503/171549742-e84edd2c-84a3-48d3-a7dd-1f1748e06087.png)


### Citing  Arabic_CUB-200-2011-Dataset
If you find our Arabic_CUB-200-2011-Dataset useful in your research, please consider citing:

```
@article{10.1145/3490504,
author = {Mathematics, Sara Maher and Loey, Mohamed},
title = {Photo Realistic Generation from Arabic Text Description Based on Generative Adversarial Networks},
year = {2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3490504},
doi = {10.1145/3490504},
abstract = {Generating accurate high-resolution images from text representations is a difficult problem in computer vision that has a wide range of functional applications. Text-to-image conversion is not dissimilar to the difficulties inherent in language processing. For example, each text meaning can be encoded in two distinct human languages, while photographs and text are two distinct encoding languages for similar data. However, these are two distinct issues, since text-to-image or image-to-text conversions are extremely multimodal in nature. The proposed model for creating 256 \texttimes{} 256 realistic images from Arabic text descriptions is discussed in this article. The relationship between an Arabic word in a sentence and its component in a picture as introduced in this paper using the DAMSM model. This model teaches two neural networks how to map the Arabic picture and word sub-regions of a full sentence to a shared semantic model. It performs well as an Arabic-text encoder and a picture encoder. We start with the Modified-Arabic dataset and train the model from scratch. The proposed model establishes a new standard for the conversion of Arabic text to realistic pictures. A mutation happens when Arabic is used as the primary language for converting Arabic texts to real images. The inception score of the newly introduced model reported by 3.42 ± .05 on the CUB database.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {sep},
keywords = {Generative Adversarial Networks (GANs), Natural Language Processing (NLP), Text-to-Image Synthesis, Image-realistic generation}
}
```
